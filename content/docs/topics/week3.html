<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 1 - Bayesian Optimization</title>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
</head>
<body>

<h1>Week 1</h1>

<p><strong>Topic:</strong> Neural Tangent Kernel </p>
<p><strong>Keynote Speaker:</strong> Xunjian Li</p>
<p><strong>Time:</strong> Aug 5, 20:00 - 21:30</p>
<p><strong>Venue:</strong> Room 338, School of Business</p>
<p><strong>Tencent Meeting:</strong> #907-2153-6929</p>

<h2>Compendium</h2>

<p>The Neural Tangent Kernel (NTK) is a crucial concept in understanding the behavior of neural networks, particularly in the infinite width limit. It reveals that as the width of a neural network increases, its dynamics can be approximated as linear, allowing for closed-form solutions to training dynamics and predictions. The NTK serves as a non-linear transformation of input data, linking neural networks to kernel methods. Key topics include the gradient flow, initialization strategies, and both empirical and analytical formulations of the NTK for shallow and deep networks. This framework provides insights into the trainability and convergence properties of neural networks, bridging theoretical and practical aspects of machine learning.</p>



<h2>Material</h2>

<p>Presentation Slides: <a href="https://wma17.github.io/24summer/docs/pdfs/Week3_NTK.pdf">https://wma17.github.io/24summer/docs/pdfs/Week3_NTK.pdf</a></p>
<iframe src="https://wma17.github.io/24summer/docs/pdfs/Week3_NTK.pdf" width="100%" height="600px"></iframe>

<h2>References</h2>
<ol>
  <li>Cho, Y., & Saul, L. (2009). Kernel methods for deep learning. Advances in neural information processing systems, 22.</li>
  <li>Golikov, E., Pokonechnyy, E., & Korviakov, V. (2022). Neural tangent kernel: A survey. arXiv preprint arXiv:2208.13614.</li>
  <li>He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision (pp. 1026-1034).</li>
  <li>Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J., & Pennington, J. (2019). Wide neural networks of any depth evolve as linear models under gradient descent. Advances in neural information processing systems, 32.</li>
  <li>Liu, C., Zhu, L., & Belkin, M. (2020). On the linearity of large non-linear models: when and why the tangent kernel is constant. Advances in Neural Information Processing Systems, 33, 15954â€“15964.</li>
  <li>Neal, R. M., & Neal, R. M. (1996). Priors for infinite networks. Bayesian learning for neural networks, 29-53</li>
</ol>

</body>
</html>
