<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Week1</title>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
</head>
<body>

<h1>Week 1</h1>

<p><strong>Topic:</strong> Bayesian Optimization</p>
<p><strong>Keynote Speaker:</strong> Wang Ma</p>
<p><strong>Time:</strong> Jul 22, 20:00 - 21:30 pm</p>
<p><strong>Venue:</strong> Room 314, School of Business</p>
<p><strong>Tencent Meeting:</strong> #907-2153-6929</p>

<h2>Compendium</h2>

<h3>Introduction</h3>
<p>Bayesian Optimization (BO) is a machine-learning-based method for optimizing expensive, noisy, and black-box functions without gradient information. It excels in high-dimensional spaces (typically dimension is less than 20) and is used in applications like hyperparameter optimization, algorithm tuning, and materials discovery.</p>

<h3>1. Problem Setup</h3>
<p>BO aims to solve:</p>
<p>\[\hat{x} = \max_{x \in A} f(x)\]</p>
<p>where f is continuous, derivative-free, expensive, and possibly noisy.</p>

<h3>2. Bayesian Optimization Algorithm</h3>
<ol>
  <li>Assume a Bayesian prior on \( f \).</li>
  <li>Iteratively:
    <ul>
      <li>Maximize the acquisition function to find the next point.</li>
      <li>Evaluate the function at this point.</li>
      <li>Update the posterior distribution on \( f \).</li>
    </ul>
  </li>
</ol>

<h3>3. Gaussian Process Model</h3>
<p>A Gaussian Process (GP) models \( f \) with a mean function \( m(x) \) and a covariance function \( k(x, x') \):</p>
<ul>
  <li>Mean: \[m(x) = \mathbb{E}[f(x)]\]</li>
  <li>Covariance: \[k(x, x') = \mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\]</li>
</ul>

<h4>Common Covariance Functions</h4>
<ul>
  <li>Squared Exponential</li>
  <li>Matern</li>
  <li>Rational Quadratic</li>
</ul>

<h4>Prediction</h4>
<p>With observations \[ f = [f(x_1), f(x_2), \ldots, f(x_t)], \] the prediction at new point will be joint normal:</p>
<p>\[\mathbb{P}\left( \begin{bmatrix} f \\ f^* \end{bmatrix} \right) = \mathcal{N} \left( 0, \begin{bmatrix} K[X, X] & K[X, x^*] \\ K[x^*, X] & K[x^*, x^*] \end{bmatrix} \right)\]</p>
<p> Then with the property of Multivariate Normal Distribution, we wil get the conditional distribution \[ \mathbb{P} (x^* | X) \ ], whose mean and variance has computable closed form. <p> 
<h3>4. Acquisition Functions</h3>
<p>Acquisition functions balance exploration and exploitation to select the next evaluation point.</p>

<h4>Common Acquisition Functions</h4>
<ul>
  <li><strong>Upper Confidence Bound (UCB)</strong></li>
  <li><strong>Probability of Improvement (PI)</strong></li>
  <li><strong>Expected Improvement (EI)</strong></li>
</ul>

<h4>Parallel Acquisition Functions</h4>
<ul>
  <li><strong>Parallel Expected Improvement (PEI)</strong></li>
  <li><strong>Knowledge Gradient (KG)</strong></li>
</ul>

<h3>Conclusion</h3>
<p>Bayesian Optimization effectively optimizes expensive and noisy functions by leveraging Gaussian Processes and acquisition functions to balance exploration and exploitation. It has broad applications in various fields requiring efficient optimization techniques.</p>

<h2>Material</h2>
  
<iframe src="https://wma17.github.io/24summer/docs/pdfs/Week1_BayesOPT.pdf" width="100%" height="600px"></iframe>
  

<h2>References</h2>
<ol>
  <li><a href="https://www.borealisai.com/research-blogs/tutorial-8-bayesian-optimization/">UDL book further reading about Bayesian Optimization</a></li>
  <li><a href="https://people.orie.cornell.edu/pfrazier/Presentations/2018.11.INFORMS.tutorial.pdf">Prof. Peter Frazier's talk about BO</a></li>
  <li><a href="https://tiao.io/post/an-illustrated-guide-to-the-knowledge-gradient-acquisition-function/">Louis Tiao's introduction to Knowledge Gradient</a></li>
  <li>V. Nguyen, "Bayesian Optimization for Accelerating Hyper-Parameter Tuning," 2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering (AIKE), Sardinia, Italy, 2019, pp. 302-305, doi: 10.1109/AIKE.2019.00060.</li>
  <li>Frazier, P. I. (2018). A Tutorial on Bayesian Optimization. arXiv preprint arXiv:1807.02811</li>
  <li>Wang, J., Clark, S. C., Liu, E., & Frazier, P. I. (2016). Parallel Bayesian global optimization of expensive functions. arXiv preprint arXiv:1602.05149.</li>
</ol>

</body>
</html>
